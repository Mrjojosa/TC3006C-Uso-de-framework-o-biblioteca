# -*- coding: utf-8 -*-
"""Retroalimentacion framework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pl_jtiHdM5qqpOfnudpeMHGDSxCXqN4Q
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/drive")
!pwd # Print working directory

# %cd "/content/drive/MyDrive/ITA/Machine learning"
!ls # List files located in defined folder

#Importar librerías
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

#Leer csv
ds = pd.read_csv('Global YouTube Statistics.csv', encoding='latin1')
ds

#El csv contiene, como su nombre lo indica, estadísticas de youtube, este modelo de predicción buscará predecir de manera optima la cantidad de vistas en los videos de canales de youtube

ds.isna().sum()

#Checamos la correlación de los valores numéricos
ds.corr()

#Elimino la mayoría de los datos que pienso que no me servirían mucho y los datos redundantes
ds = ds.drop(["Title","Abbreviation","channel_type","video_views_rank","country_rank","lowest_yearly_earnings","highest_yearly_earnings","created_date","Gross tertiary education enrollment (%)","Population","Unemployment rate","Urban_population","Latitude","Longitude"],axis=1,errors="ignore")
ds

#Verifico que las variables dummies de paises tengan por lo menos cierto nivel de correlación en alguna
cdumm = pd.get_dummies(ds.Country)
cdumm['video_views'] = ds.video_views
cdumm.corr()

#Al ver que no las desecho
ds = ds.drop("Country",axis=1,errors="ignore")
ds

#Hago lo mismo con las categorías de contenido
catdumm = pd.get_dummies(ds.category)
catdumm['video_views'] = ds.video_views
catdumm.corr()

#Las veo con mayor correlación e importancia en algunos casos, por lo que guardo algunas
cardumm = catdumm.drop(["video_views","category"],axis=1,errors="ignore")
catdumm2 = catdumm.drop(["Travel & Events	","Trailers","Sports","Pets & Animals","Nonprofits & Activism","News & Politics","Movies","Film & Animation","Entertainment","Autos & Vehicles","category","video_views"],axis=1,errors="ignore")
newds = ds.join(catdumm2)
newds.corr()

y = ds['video_views']
y.isna().sum()
newds.isna().sum()

#Ahora paso a quitar los datos nulos reemplazandolos con la media, y en el caso de la cantidad de subscriptores de los ultimos 30 días elimine el feature por que un tercio de sus datos son nulos. También categoría porque se cambió con los dummies
from pandas.core.nanops import nanmean
newds = newds.drop(["subscribers_for_last_30_days","category","Youtuber"],axis=1,errors="ignore")
newds['created_month'] = newds['created_month'].map({"Jan":1,
"Feb":2, "Mar":3, "Apr":4, "May":5, "Jun":6, "Jul":7, "Aug":8, "Sep":9, "Oct":10, "Nov":11, "Dec":12})
mcreatedm = nanmean(newds['created_month'])
mcreatedy = nanmean(newds['created_year'])
mchanneltr = nanmean(newds['channel_type_rank'])
mvvftl30d = nanmean(newds['video_views_for_the_last_30_days'])
newds['created_month'].fillna(mcreatedm, inplace = True)
newds.isna().sum()

newds['created_year'].fillna(mcreatedy, inplace = True)
newds.isna().sum()

newds['channel_type_rank'].fillna(mchanneltr, inplace = True)
newds.isna().sum()

newds['video_views_for_the_last_30_days'].fillna(mvvftl30d, inplace = True)
newds.isna().sum()

#Con los datos listos ahora es momento de comenzar el método
#Primero hay que dividir el dataset en muestra de entrenamiento, validación y de testeo
X_train, X_test, y_train, y_test = train_test_split(newds, y)

#Se introduce la función ya hecha para el random forest
RF = RandomForestClassifier()

#Se definen los varios parametros para el random forest de manera que pueda elegir cual trae el mejor resultado
param_grid = [{'n_estimators': [10, 100, 200, 500, 1000], 'max_depth': [5,10], 'min_samples_split': [2,3,4,5], 'n_jobs': [10]}]
grid_search = GridSearchCV(RF, param_grid, cv=3, scoring='accuracy', return_train_score=True )

#Se entrena el modelo con su mejor estimación y se revisa el resultado siendo 1 el mejor
grid_search.fit(X_train, y_train)
bestRF = grid_search.best_estimator_
bestRF.score(X_train, y_train)

